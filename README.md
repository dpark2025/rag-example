# Local RAG System ğŸ¤–

A **fully local RAG (Retrieval-Augmented Generation) system** that runs completely offline using Docker containers. This system combines efficient document retrieval with local LLM generation to create an intelligent question-answering system that processes your documents without external API calls.

## ğŸš€ Quick Start

### Option 1: Automated Setup
```bash
# Clone or navigate to project directory
cd rag-example

# Run automated setup
chmod +x setup.sh && ./setup.sh
```

### Option 2: Using Makefile
```bash
make setup
```

### Option 3: Manual Setup
```bash
# Build containers
docker-compose build

# Start services
docker-compose up -d

# Download initial model
docker-compose exec ollama ollama pull llama3.2:3b
```

## ğŸŒ Access Your System

- **Streamlit UI**: http://localhost:8501 (primary interface)
- **FastAPI Docs**: http://localhost:8000/docs (API documentation)
- **Ollama API**: http://localhost:11434 (LLM server)

## âœ¨ Features

- âœ… **Completely Offline** - No internet required after setup
- âœ… **Data Privacy** - Everything stays on your machine
- âœ… **Smart Chunking** - Semantic document splitting for optimal retrieval
- âœ… **Adaptive Retrieval** - Efficiency-optimized context selection
- âœ… **Multiple LLMs** - Switch between different Ollama models
- âœ… **Real-time Metrics** - Token usage and efficiency monitoring
- âœ… **Web Interface** - User-friendly Streamlit UI
- âœ… **REST API** - Full FastAPI backend with auto-documentation
- âœ… **MCP Protocol** - Standardized tool interface
- âœ… **Docker Containerized** - Easy deployment and scaling

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚â—„â”€â”€â–ºâ”‚   FastAPI        â”‚â—„â”€â”€â–ºâ”‚   ChromaDB      â”‚
â”‚   Web UI        â”‚    â”‚   REST API       â”‚    â”‚   Vector Store  â”‚
â”‚   Port: 8501    â”‚    â”‚   Port: 8000     â”‚    â”‚   Local Storage â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚                        â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Ollama LLM    â”‚
                        â”‚   Port: 11434   â”‚
                        â”‚   Local Models  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components
- **Streamlit UI**: Document management and chat interface
- **FastAPI Backend**: REST API with automatic documentation
- **RAG Engine**: Smart chunking and adaptive retrieval
- **MCP Server**: Model Context Protocol for tool integration
- **Ollama**: Local LLM server supporting multiple models
- **ChromaDB**: Local vector database with persistence

## ğŸ“Š Performance Optimizations

### Smart Chunking Strategy
- **Semantic Boundaries**: Splits by paragraphs/sentences, not arbitrary tokens
- **Optimal Size**: 400 tokens per chunk for best retrieval performance
- **Context Overlap**: 50-token overlap for continuity
- **Edge Case Handling**: Manages short documents and large paragraphs

### Adaptive Retrieval System
- **Relevance Filtering**: Only retrieves chunks above 0.7 similarity threshold
- **Token Management**: Stops at context limits to prevent overflow
- **Quality First**: Prioritizes high-relevance chunks over quantity
- **Query Analysis**: Adapts chunk count based on query complexity

### Efficiency Metrics
- **Real-time Monitoring**: Track token usage and response times
- **Configurable Limits**: Adjust similarity thresholds and context windows
- **Performance Ratios**: Monitor chunks-per-token efficiency

## ğŸ”§ Available Models

```bash
# Small models (3-4GB RAM)
make pull-model MODEL=llama3.2:3b
make pull-model MODEL=phi3:mini

# Medium models (8GB RAM)
make pull-model MODEL=llama3.2:8b
make pull-model MODEL=mistral:7b

# Large models (16GB+ RAM)
make pull-model MODEL=llama3.1:70b
make pull-model MODEL=codellama:34b
```

## ğŸ’¾ Resource Requirements

| Component | Minimum RAM | Recommended RAM |
|-----------|-------------|-----------------|
| RAG App | 2GB | 4GB |
| Ollama + 3B Model | 4GB | 8GB |
| Ollama + 8B Model | 8GB | 16GB |
| **Total System** | **6GB** | **20GB** |

## ğŸ› ï¸ Development Commands

### Container Management
```bash
make build          # Build all containers
make start          # Start services
make stop           # Stop services
make logs           # View logs
make health         # Check system health
```

### Shell Access
```bash
make shell-rag      # Access RAG app container
make shell-ollama   # Access Ollama container
```

### Testing
```bash
# Run system integration tests
python scripts/test_system.py

# Run performance benchmarks
python scripts/benchmark.py
```

## ğŸ“š Usage Examples

### Document Upload via UI
1. Navigate to http://localhost:8501
2. Use sidebar "Add Documents" or "Bulk Upload"
3. Start asking questions in the chat interface

### Document Upload via API
```python
import requests

documents = [
    {
        "title": "My Document",
        "content": "Document content here...",
        "source": "api_upload"
    }
]

response = requests.post(
    "http://localhost:8000/documents",
    json=documents
)
```

### Query via API
```python
response = requests.post(
    "http://localhost:8000/query",
    json={
        "question": "What is machine learning?",
        "max_chunks": 3
    }
)

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")
```

## ğŸ§ª Testing

### System Integration Tests
```bash
python scripts/test_system.py
```
Tests all major functionality:
- Health checks
- Document upload
- Query processing
- Settings management

### Performance Benchmarking
```bash
python scripts/benchmark.py
```
Measures:
- Response times
- Token efficiency
- Concurrent query performance
- Throughput metrics

## ğŸ” Troubleshooting

### Check System Status
```bash
make health
docker-compose ps
```

### View Logs
```bash
make logs                    # All services
docker-compose logs ollama   # Ollama only
docker-compose logs rag-app  # RAG app only
```

### Common Issues

**Ollama not responding:**
```bash
docker-compose restart ollama
docker-compose exec ollama ollama list  # Check available models
```

**Out of memory:**
- Use smaller models (3B instead of 8B)
- Reduce `max_context_tokens` in settings
- Increase Docker memory limits

**Slow responses:**
- Check if model is fully loaded: `docker-compose logs ollama`
- Reduce similarity threshold for more results
- Use faster models like `phi3:mini`

### Clean Restart
```bash
docker-compose down
docker-compose up -d
```

## ğŸ“ Project Structure

```
rag-example/
â”œâ”€â”€ app/                    # Python application
â”‚   â”œâ”€â”€ main.py            # FastAPI REST API
â”‚   â”œâ”€â”€ rag_backend.py     # RAG engine
â”‚   â”œâ”€â”€ mcp_server.py      # MCP protocol server
â”‚   â”œâ”€â”€ agent_ui.py        # Streamlit UI
â”‚   â””â”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ data/                   # Persistent storage
â”‚   â”œâ”€â”€ chroma_db/         # Vector database
â”‚   â””â”€â”€ documents/         # Document uploads
â”œâ”€â”€ models/                 # Model storage
â”‚   â””â”€â”€ ollama_models/     # Ollama model files
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ test_system.py     # Integration tests
â”‚   â””â”€â”€ benchmark.py       # Performance tests
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ agents/                # Agent configurations
â”œâ”€â”€ planning/              # Project planning
â”œâ”€â”€ docker-compose.yml     # Service orchestration
â”œâ”€â”€ Dockerfile.rag-app     # RAG app container
â”œâ”€â”€ Dockerfile.ollama      # Ollama container
â”œâ”€â”€ setup.sh              # Automated setup
â”œâ”€â”€ Makefile              # Development commands
â””â”€â”€ README.md             # This file
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `python scripts/test_system.py`
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for local LLM serving
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [Sentence Transformers](https://www.sbert.net/) for embeddings
- [Streamlit](https://streamlit.io/) for the web interface
- [FastAPI](https://fastapi.tiangolo.com/) for the REST API